# ‚úÖ utils.py - ‡πÑ‡∏°‡πà import torch ‡∏ó‡∏µ‡πà top level

from PIL import Image
import io
import numpy as np
import os
import logging

logger = logging.getLogger(__name__)

HF_TOKEN = os.getenv("HF_TOKEN")
finetuned_repo = "freemanlnwza/modelCLIPfine-tuned"

# ‚ùå ‡∏≠‡∏¢‡πà‡∏≤‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ (Top-level import)
# import torch                                    ‚Üê ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å!
# from transformers import CLIPProcessor, CLIPModel  ‚Üê ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å!
# device = "cuda" if torch.cuda.is_available() else "cpu"  ‚Üê ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å!

# ‚úÖ Global cache (Lazy load)
_finetuned_processor = None
_finetuned_model = None
_device = None

# ===============================
# Helper function - ‡∏î‡∏∂‡∏á device
# ===============================
def get_device():
    """‚úÖ ‡∏î‡∏∂‡∏á device (lazy load torch)"""
    global _device
    
    if _device is not None:
        return _device
    
    try:
        import torch
        _device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üì± Using device: {_device}")
        return _device
    except Exception as e:
        logger.error(f"‚ùå Error detecting device: {e}")
        return "cpu"

# ===============================
# Lazy Load CLIP Model
# ===============================
def get_finetuned_clip():
    """‚úÖ ‡∏î‡∏∂‡∏á fine-tuned CLIP model (lazy load)"""
    global _finetuned_processor, _finetuned_model
    
    # ‡∏ñ‡πâ‡∏≤ load ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ return cache
    if _finetuned_processor is not None and _finetuned_model is not None:
        return _finetuned_processor, _finetuned_model
    
    if not HF_TOKEN:
        logger.error("‚ùå HF_TOKEN not set")
        raise ValueError("HF_TOKEN not configured")
    
    try:
        logger.info("üì• Loading fine-tuned CLIP from Hugging Face...")
        
        # ‚úÖ Import torch ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (lazy load!)
        import torch
        from transformers import CLIPProcessor, CLIPModel
        
        device = get_device()
        
        _finetuned_processor = CLIPProcessor.from_pretrained(
            finetuned_repo,
            token=HF_TOKEN
        )
        
        _finetuned_model = CLIPModel.from_pretrained(
            finetuned_repo,
            token=HF_TOKEN
        ).to(device)
        
        logger.info("‚úÖ Fine-tuned CLIP loaded from HF successfully")
        return _finetuned_processor, _finetuned_model
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load fine-tuned CLIP: {str(e)}")
        _finetuned_processor = None
        _finetuned_model = None
        raise

# ===============================
# Get Text Embedding
# ===============================
def get_text_embedding(text: str) -> np.ndarray:
    """
    ‚úÖ ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ embedding ‡πÄ‡∏õ‡πá‡∏ô numpy array
    - Import torch ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    """
    if not text:
        logger.warning("‚ö†Ô∏è  Empty text provided")
        return None
    
    try:
        # ‚úÖ Import torch ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        import torch
        
        processor, model = get_finetuned_clip()
        
        logger.info(f"üß† Generating text embedding for: {text[:50]}...")
        
        inputs = processor(text=[text], return_tensors="pt", padding=True)
        
        with torch.no_grad():
            embeddings = model.get_text_features(**inputs)
            logger.debug(f"[INFO] Embedding shape: {embeddings.shape}")
        
        result = embeddings[0].cpu().numpy()
        logger.info(f"‚úÖ Text embedding generated (dim: {len(result)})")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error generating text embedding: {str(e)}")
        return None

# ===============================
# Get Image Embedding
# ===============================
def get_image_embedding(image_source) -> np.ndarray:
    """
    ‚úÖ ‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö:
      - path (str)
      - UploadFile (FastAPI)
      - bytes ‡∏´‡∏£‡∏∑‡∏≠ io.BytesIO
    
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ embedding ‡πÄ‡∏õ‡πá‡∏ô numpy array
    - Import torch ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    """
    try:
        # ‚úÖ Import torch ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        import torch
        
        processor, model = get_finetuned_clip()
        
        # ‚úÖ Parse image source
        if hasattr(image_source, "file"):  # UploadFile (FastAPI)
            logger.info("üì∑ Processing UploadFile...")
            image_bytes = image_source.file.read()
            image_source.file.seek(0)
            image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
            
        elif isinstance(image_source, (bytes, io.BytesIO)):  # bytes or BytesIO
            logger.info("üì∑ Processing bytes/BytesIO...")
            if isinstance(image_source, bytes):
                image_source = io.BytesIO(image_source)
            image = Image.open(image_source).convert("RGB")
            
        else:  # path (str)
            logger.info(f"üì∑ Processing image from path: {image_source}")
            image = Image.open(image_source).convert("RGB")
        
        logger.debug(f"Image size: {image.size}")
        
        inputs = processor(images=image, return_tensors="pt")
        
        with torch.no_grad():
            embeddings = model.get_image_features(**inputs)
        
        result = embeddings[0].cpu().numpy()
        logger.info(f"‚úÖ Image embedding generated (dim: {len(result)})")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error generating image embedding: {str(e)}")
        return None

# ===============================
# Validate Image Embedding
# ===============================
def validate_image_embedding(image_bytes: bytes) -> list:
    """‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö image embedding"""
    try:
        embedding = get_image_embedding(image_bytes)
        
        if embedding is None:
            logger.error("‚ùå Image embedding is None")
            raise ValueError("Image embedding is None")
        
        emb_array = np.array(embedding)
        
        if emb_array.size == 0:
            logger.error("‚ùå Image embedding is empty")
            raise ValueError("Image embedding is empty")
        
        if not np.issubdtype(emb_array.dtype, np.floating):
            logger.error("‚ùå Image embedding must be float type")
            raise ValueError("Image embedding must be float type")
        
        logger.info(f"‚úÖ Image embedding validated (shape: {emb_array.shape})")
        return emb_array.tolist()
        
    except Exception as e:
        logger.error(f"‚ùå Validation error: {str(e)}")
        return None

# ===============================
# Cosine Similarity
# ===============================
def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 vectors
    - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á torch (‡πÉ‡∏ä‡πâ numpy)
    """
    try:
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            logger.warning("‚ö†Ô∏è  Zero norm vector")
            return 0.0
        
        similarity = np.dot(vec1, vec2) / (norm1 * norm2)
        return float(similarity)
        
    except Exception as e:
        logger.error(f"‚ùå Error calculating cosine similarity: {str(e)}")
        return 0.0

# ===============================
# Startup Check
# ===============================
logger.info("[‚úÖ DEBUG] utils.py loaded successfully")
logger.info("[‚ÑπÔ∏è NOTE] Models will be lazy-loaded on first use")